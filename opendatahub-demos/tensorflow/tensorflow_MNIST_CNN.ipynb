{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c28b06a-53c6-41b9-a9e2-62fb794cd477",
   "metadata": {},
   "source": [
    "Import the necessary libraries to build the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0afd8-b5a7-4dcb-be69-551b9ed52af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, datasets\n",
    "from tensorflow.keras import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9a4eb-4b69-4cad-a33a-d14318385b5f",
   "metadata": {},
   "source": [
    "Import the dataset and split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f98e7c-5ed9-47e8-95d9-614b2d58b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_train, output_train), (input_test, output_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4984b-dbf3-4cb3-86f3-f9f00784d904",
   "metadata": {},
   "source": [
    "Reshape and normalize the input data to fit the size of the MNIST images and normalize the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49707f10-a5cc-4e72-a45c-66b38ea12963",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_train.reshape(-1, 28, 28, 1)\n",
    "input_test = input_test.reshape(-1, 28, 28, 1)\n",
    "input_train, input_test = input_train / 255.0, input_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c141537-f232-4711-8d46-11a6e4d1d063",
   "metadata": {},
   "source": [
    "Use the one_hot function to encode the labeled data to be continuous instead of categorical by converting to one_hot encoded vectors with a depth of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736030f-3782-4148-9a15-61aa9a6c934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(output_train.astype(\"float32\"), depth=10)\n",
    "tf.one_hot(output_test.astype(\"float32\"), depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262557b9-7fbc-4b83-bc72-98386d50e26b",
   "metadata": {},
   "source": [
    "Create the Convuluted Neural Network model with the Sequential class, and add all necessary layers to complete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7da5a-d827-4bd7-8d5c-7e1ff3c0d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential = Sequential()\n",
    "sequential.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)))\n",
    "sequential.add(layers.MaxPooling2D(2, 2))\n",
    "sequential.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "sequential.add(layers.MaxPooling2D(2, 2))\n",
    "sequential.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "sequential.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a96f9-e88a-4ebe-ad08-d365579db0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential.add(layers.Flatten())\n",
    "sequential.add(layers.Dense(64, activation='relu'))\n",
    "sequential.add(layers.Dense(10))\n",
    "sequential.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0e86d-5dc5-4807-89a7-632da48e9646",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compile and train the model with the training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c888669-507d-41e7-baf9-4e11cd643f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "training = sequential.fit(input_train, output_train, batch_size=128, epochs=5, validation_data=(input_test, output_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb081da-9aa2-4acf-852c-ca6027f4c437",
   "metadata": {},
   "source": [
    "Plot the accuracy of the model's training performance using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41de942-27fe-4b8b-9bf3-7df3886589da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(training.history['accuracy'], label='accuracy')\n",
    "plt.plot(training.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([.9, 1])\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82e033-d31b-4248-aba5-833cf21d6a3f",
   "metadata": {},
   "source": [
    "Evaluate the model's performance, and then calculate and return the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbf714-2920-4bd8-a3fc-5c2563354994",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = sequential.evaluate(input_test, output_test)\n",
    "print(\"Test Loss: \" ,loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2cc98-4c73-4053-8d5e-b3a6af14e717",
   "metadata": {},
   "source": [
    "Save the model in the appropriate format to build an image storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d935f-4e0a-47b3-b35f-9441735c55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential.save('tensorflow_mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec68ea-6b49-4d13-b83f-8a991d01a167",
   "metadata": {},
   "source": [
    "Get the sample data from the sample image to give the model to make a prediction. Ensure the model gives a prediction with the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04979818-1e2f-494a-9aec-965bb6ceebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    " # load the image\n",
    " img = load_img(filename, grayscale=True, target_size=(28, 28), color_mode=\"grayscale\")\n",
    " # convert to array\n",
    " img = img_to_array(img)\n",
    " # reshape into a single sample with 1 channel\n",
    " img = img.reshape(1, 28, 28, 1)\n",
    " # prepare pixel data\n",
    " img = img.astype('float32')\n",
    " img = img / 255.0\n",
    " return img\n",
    "\n",
    "image = load_image(\"sample_image.png\")\n",
    "image_list = image.tolist()\n",
    "print(image_list)\n",
    "prediction = sequential.predict(image)\n",
    "number = argmax(prediction)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b6e59-fa32-420d-9f41-9f1008613a4d",
   "metadata": {},
   "source": [
    "Login to OpenShift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
